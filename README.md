Here is the updated `README.md` tailored to your GitHub project. I have added the **Project Structure Analysis** and the **Data Disclaimer** as requested.

***

# NEEDLEINATABLE: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables

[![arXiv](https://img.shields.io/badge/arXiv-2504.06560-b31b1b.svg)](https://arxiv.org/abs/2504.06560)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository contains the official implementation and data for the paper **"NEEDLEINATABLE: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables"**, accepted at **NeurIPS 2025**.

## ðŸ“– Overview

**NEEDLEINATABLE (NIAT)** is a benchmark designed to evaluate the fine-grained perception of Large Language Models (LLMs) regarding individual table cells within long-context structured tables. Unlike previous benchmarks that focus on high-level reasoning or unstructured text, NIAT treats each cell as a "needle" to test the model's fundamental understanding of table structures.

We also introduce a **Strong2Weak** data synthesis method, using GPT-4o to generate Chain-of-Thought (CoT) training data, which significantly improves open-source model performance on both NIAT and downstream tabular tasks.

## âš ï¸ Data Availability Note

> **Note:** Due to the massive size of the original QA (Question-Answer) dataset (up to 287K questions), we have uploaded **all original raw tables** but only a **random sample of the QA pairs** in this repository for demonstration and testing purposes. 
>
> If you require the full training/testing dataset, please refer to the instructions in the `data/` folder or contact the authors.

## ðŸ“‚ Project Structure

Below is an overview of the repository structure and the function of each module:

```text
NeedleInATable/
â”œâ”€â”€ data_synthesis/                 # Data synthesis modules
â”‚   â”œâ”€â”€ synthetic_table/            # Core synthesis scripts
â”‚   â”‚   â”œâ”€â”€ build_cell-locating.py  # Script for cell location tasks
â”‚   â”‚   â””â”€â”€ build_cell-lookup.py    # Script for cell retrieval tasks
â”‚   â””â”€â”€ promptp_cot4_cell-locating.py # CoT prompts for location tasks
â”œâ”€â”€ NIAT_data/                      # Evaluation datasets
â”‚   â”œâ”€â”€ NIAT_LLM_test_data/         # Benchmarks for text-based LLMs
â”‚   â”‚   â”œâ”€â”€ cell-locating/          # Location task samples
â”‚   â”‚   â”œâ”€â”€ cell-lookup/            # Retrieval task samples
â”‚   â”‚   â””â”€â”€ NIAT_cropped_tables.json # Cropped table data (JSON)
â”‚   â”œâ”€â”€ NIAT_MLLM_test_data/        # Benchmarks for Vision-Language Models
â”‚   â”‚   â”œâ”€â”€ cropped_table_images/   # Cropped table screenshots
â”‚   â”‚   â”œâ”€â”€ table_images/           # Full table screenshots
â”‚   â”‚   â”œâ”€â”€ vlm_cell_locating.json  # VLM location annotations
â”‚   â”‚   â”œâ”€â”€ vlm_cell_locating_50.json # VLM location subset (50 samples)
â”‚   â”‚   â””â”€â”€ vlm_cell_lookup.json    # VLM retrieval annotations
â”‚   â””â”€â”€ 360_NIAT_tables.json        # Main source table collection
â””â”€â”€ README.md                       # Project documentation
```


## ðŸ“œ Citation

If you use this code or dataset in your research, please cite our paper:

```bibtex
@inproceedings{wang2025needleinatable,
  title={NEEDLEINATABLE: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables},
  author={Wang, Lanrui and Zheng, Mingyu and Tang, Hongyin and Lin, Zheng and Cao, Yanan and Wang, Jingang and Cai, Xunliang and Wang, Weiping},
  booktitle={39th Conference on Neural Information Processing Systems (NeurIPS 2025)},
  year={2025}
}
```

## ðŸ“§ Contact

For inquiries regarding the full dataset or the codebase, please contact:

*   **Lanrui Wang:** oliveerwang@tencent.com
*   **Mingyu Zheng:** zhengmingyu@iie.ac.cn
*   **Zheng Lin:** linzheng@iie.ac.cn

**Paper Link:** [arXiv:2504.06560](https://arxiv.org/abs/2504.06560)